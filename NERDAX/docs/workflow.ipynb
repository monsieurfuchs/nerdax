{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('3.9.0')",
   "metadata": {
    "interpreter": {
     "hash": "36071112a161297f2fd106003050184fbdff34ed057f375faa6d2f5f0cad40eb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Workflow Examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`NERDA` offers a simple easy-to-use interface for fine-tuning transformers for Named-Entity Recognition (=NER).  We call this family of models `NERDA` models.\n",
    "\n",
    "`NERDA` can be used in two ways. You can either (1) train your own customized `NERDA` model or (2) download and use one of our precooked `NERDA` models for inference i.e. identifying named entities in new texts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Train Your Own `NERDA` model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We want to fine-tune a transformer for English. \n",
    "\n",
    "First, we download an English NER dataset [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) with annotated Named Entities, that we will use for training and evaluation of our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NERDA'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-920b857cbb44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# don't print warnings for this session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNERDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dane_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dane_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdownload_dane_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NERDA'"
     ]
    }
   ],
   "source": [
    "from NERDA.datasets import get_conll_data, download_conll_data\n",
    "download_conll_data()"
   ]
  },
  {
   "source": [
    "CoNLL-2003 operates with the following types of named entities:\n",
    "\n",
    "1. **PER**sons \n",
    "2. **ORG**anizations \n",
    "3. **LOC**ations \n",
    "4. **MISC**ellaneous \n",
    "5. **O**utside (Not a named Entity)\n",
    "\n",
    "An observation from the CoNLL-2003 data set looks like this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_dane_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0e6c3d5c6962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dane_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dane_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_dane_data' is not defined"
     ]
    }
   ],
   "source": [
    "# extract the first _5_ rows from the training and validation data splits.\n",
    "training = get_conll_data('train', 5)\n",
    "validation = get_conll_data('valid', 5)\n",
    "# example\n",
    "sentence = training.get('sentences')[0]\n",
    "tags = training.get('tags')[0]\n",
    "print(\"\\n\".join([\"{}/{}\".format(word, tag) for word, tag in zip(sentence, tags)]))"
   ]
  },
  {
   "source": [
    "If you provide your own dataset, it must have the same structure:\n",
    "\n",
    "- It must be a dictionary\n",
    "- The dictionary must contain\n",
    "    - 'sentences': a list of word-tokenized sentences with one sentence per entry \n",
    "    - 'tags': a list with the corresponding named-entity tags.\n",
    "\n",
    "The data set does however *not* have to follow the Inside-Outside-Beginning (IOB) tagging scheme<sup>[1]</sup>.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n",
    "\n",
    "The IOB tagging scheme implies, that words that are beginning of named entities are tagged with *'B-'* and words 'inside' (=continuations of) named entities are tagged with *'I-'*. That means that 'Joe Biden' should be tagged as `Joe(B-PER) Biden(I-PER)`.\n",
    "\n",
    "Now, instantiate a `NERDA` model for finetuning an [ELECTRA](https://huggingface.co/google/electra-small-discriminator) transformer for NER. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NERDA.models import NERDA\n",
    "tag_scheme = ['B-PER',\n",
    "              'I-PER', \n",
    "              'B-ORG', \n",
    "              'I-ORG', \n",
    "              'B-LOC', \n",
    "              'I-LOC', \n",
    "              'B-MISC', \n",
    "              'I-MISC']\n",
    "model = NERDA(dataset_training = training,\n",
    "              dataset_validation = validation,\n",
    "              tag_scheme = tag_scheme,\n",
    "              tag_outside = 'O',\n",
    "              transformer = 'google/electra-small-discriminator',\n",
    "              hyperparameters = {'epochs' : 1,\n",
    "                                 'warmup_steps' : 10,\n",
    "                                 'train_batch_size': 5,\n",
    "                                 'learning_rate': 0.0001},)"
   ]
  },
  {
   "source": [
    "Note, this model configuration only uses 5 sentences for model training to minimize execution time. Also the hyperparameters for the model have been chosen in order to minimize execution time. Therefore this example only serves to illustrate the functionality i.e. the resulting model will suck.\n",
    "\n",
    "By default the network architecture is analogous that of the models in [Hvingelby et al. 2020](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.565.pdf). \n",
    "\n",
    "The model can be trained right away by invoking the `train` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "model.train()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "We can compute the performance of the model on a test set (limited to 5 sentences):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_conll_data('test', 5)\n",
    "model.evaluate_performance(test)"
   ]
  },
  {
   "source": [
    "Unsurprisingly, the model sucks in this case due to the ludicrous specification.\n",
    "\n",
    "Named Entities in new texts can be predicted with `predict` functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Old MacDonald had a farm\"\n",
    "model.predict_text(text)"
   ]
  },
  {
   "source": [
    "Needless to say the predicted entities for this model are nonsensical.\n",
    "\n",
    "To get a more reasonable model, provide more data and a more meaningful model specification.\n",
    "\n",
    "In general `NERDA` has the following handles, that you use.\n",
    "\n",
    "1. provide your own data set \n",
    "2. choose whatever pretrained transformer you would like to fine-tune\n",
    "3. provide your own set of hyperparameters and lastly\n",
    "4. provide your own `torch` network (architecture). You can do this by instantiating a `NERDA` model with the parameter 'network' set to your own network (torch.nn.Module)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Use a Precooked `NERDA` model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have precooked a number of `NERDA` models, that you can download \n",
    "and use right off the shelf. \n",
    "\n",
    "Here is an example.\n",
    "\n",
    "Instantiate a `NERDA` model based on the English [ELECTRA](https://huggingface.co/google/electra-small-discriminator) transformer, that has been finetuned for NER in English,\n",
    "`EN_ELECTRA_EN`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from NERDA.precooked import EN_ELECTRA_EN\n",
    "model = EN_ELECTRA_EN()\n",
    "\n"
   ]
  },
  {
   "source": [
    "(Down)load network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.download_network()\n",
    "model.load_network()\n"
   ]
  },
  {
   "source": [
    "This model performs much better:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_performance(get_conll_data('test', 100))"
   ]
  },
  {
   "source": [
    "Predict named entities in new texts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Old MacDonald had a farm'\n",
    "model.predict_text(text)\n"
   ]
  },
  {
   "source": [
    "### List of Precooked Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The table below shows the precooked `NERDA` models publicly available for download. We have trained models for Danish and English.\n",
    "\n",
    "\n",
    "| **Model**       | **Language** | **Transformer**   | **Dataset** | **F1-score** |  \n",
    "|-----------------|--------------|-------------------|---------|-----|\n",
    "| `DA_BERT_ML`    | Danish       | [Multilingual BERT](https://huggingface.co/bert-base-multilingual-uncased) | [DaNE](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane) | 82.8  | \n",
    "`DA_ELECTRA_DA` | Danish       | [Danish ELECTRA](https://huggingface.co/Maltehb/-l-ctra-danish-electra-small-uncased) | [DaNE](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane) | 79.8             |\n",
    "| `EN_BERT_ML`    | English      | [Multilingual BERT](https://huggingface.co/bert-base-multilingual-uncased)| [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) | 90.4              |\n",
    "| `EN_ELECTRA_EN` | English       | [English ELECTRA](https://huggingface.co/google/electra-small-discriminator) | [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) | 89.1             |\n",
    "\n",
    "**F1-score** is the micro-averaged F1-score across entity tags and is \n",
    "evaluated on the respective test sets (that have not been used for training nor\n",
    "validation of the models).\n",
    "\n",
    "Note, that we have not spent a lot of time on actually fine-tuning the models,\n",
    "so there could be room for improvement. If you are able to improve the models,\n",
    "we will be happy to hear from you and include your `NERDA` model.\n",
    "\n",
    "#### Performance of Precooked Models\n",
    "\n",
    "The table below summarizes the performance as measured by F1-scores of the model\n",
    " configurations, that `NERDA` ships with. \n",
    "\n",
    "| **Level**     | `DA_BERT_ML` | `DA_ELECTRA_DA` | `EN_BERT_ML` | `EN_ELECTRA_EN` |\n",
    "|---------------|-----------|------------|-------------|----------------|\n",
    "| B-PER         | 93.8      | 92.0       | 96.0        | 95.1           |      \n",
    "| I-PER         | 97.8      | 97.1       | 98.5        | 97.9           |   \n",
    "| B-ORG         | 69.5      | 66.9       | 88.4        | 86.2           |     \n",
    "| I-ORG         | 69.9      | 70.7       | 85.7        | 83.1           |   \n",
    "| B-LOC         | 82.5      | 79.0       | 92.3        | 91.1           |     \n",
    "| I-LOC         | 31.6      | 44.4       | 83.9        | 80.5           |     \n",
    "| B-MISC        | 73.4      | 68.6       | 81.8        | 80.1           |     \n",
    "| I-MISC        | 86.1      | 63.6       | 63.4        | 68.4           |   \n",
    "| **AVG_MICRO** | 82.8      | 79.8       | 90.4        | 89.1           |      \n",
    "| **AVG_MACRO** | 75.6      | 72.8       | 86.3        | 85.3           |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This concludes our walkthrough of `NERDA`. If you have any questions, please do not hesitate to [contact us](mailto:lars.kjeldgaard@eb.dk)!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}